"""
SQH Service
Handles Secondary Query Handler logic:
1. Generates Task Plan using LLM
2. Registers tasks with Orchestrator
3. Triggers Execution Engine
"""

import logging
import json
import asyncio
from typing import List, Dict, Any

from app.core.models import LifecycleMessages, Task
from app.models.pqh_response_model import PQHResponse
from app.prompts.sqh_prompt import build_sqh_prompt
from app.ai.providers.manager import ProviderManager
from app.core.orchestrator import get_orchestrator
from app.core.execution_engine import get_execution_engine
from app.core.server_executor import get_server_executor
from app.core.task_emitter import get_task_emitter
from app.config import settings

logger = logging.getLogger(__name__)

async def process_sqh(
    # pqh_response: PQHResponse,
    # user_details: Dict[str, Any]
) -> None:
    """
    Process SQH in background:
    - Generate Plan
    - Register Tasks
    - Trigger Execution
    """
    # user_id = user_details.get("id", "guest")
    # # user_details['id'] might be ObjectId, ensure string
    # if not isinstance(user_id, str):
    #     user_id = str(user_id)

    # logger.info(f"ğŸš€ [SQH] Starting background task generation for user: {user_id}")

    try:
        # 1. Build Prompt
        # prompt = build_sqh_prompt(pqh_response, user_details)
        
        # # 2. Call AI (Reasoning Model)
        # provider_manager = ProviderManager(user_details)
        
        # # Use a reasoning model or smart model for planning
        # model_name = settings.openrouter_reasoning_model_name
        
        # logger.info(f"ğŸ§  [SQH] calling LLM ({model_name})...")
        # raw_response, provider = await provider_manager.call_with_fallback(
        #     prompt=prompt
        # )
        
        # if not raw_response:
        #     logger.error("âŒ [SQH] Empty response from LLM")
        #     return
        
        # # 3. Parse Response
        # try:
        #     # Clean markdown code blocks if present
        #     cleaned_json = raw_response.strip()
        #     if cleaned_json.startswith("```json"):
        #         cleaned_json = cleaned_json.split("\n", 1)[1]
        #     if cleaned_json.endswith("```"):
        #         cleaned_json = cleaned_json.rsplit("\n", 1)[0]
            
        #     data = json.loads(cleaned_json)
            
        #     # Handle both {"tasks": [...]} and [...] formats
        #     if isinstance(data, list):
        #         tasks_data = data
        #     elif isinstance(data, dict):
        #         tasks_data = data.get("tasks", [])
        #     else:
        #         logger.error(f"âŒ [SQH] Invalid JSON format: {type(data)}")
        #         return
            
        #     logger.info(f"SQH response:\n{json.dumps(tasks_data, indent=2)}")
        #     # Parse into Task objects
        #     tasks = [Task(**task_data) for task_data in tasks_data]
            
        #     if not tasks:
        #         logger.warning("âš ï¸ [SQH] No tasks generated by LLM")
        #         return
            
        #     logger.info(f"ğŸ“‹ [SQH] Parsed {len(tasks)} tasks:")
        #     for task in tasks:
        #         logger.info(f"   - {task.task_id}: {task.tool} ({task.execution_target})")

        # except json.JSONDecodeError as e:
        #     logger.error(f"âŒ [SQH] JSON Parse Error: {e}")
        #     logger.debug(f"Raw response: {raw_response}")
        #     return
        # except Exception as e:
        #     logger.error(f"âŒ [SQH] Task Validation Error: {e}")
        #     return
        
        user_id = "guest"
    
        tasks = [
            Task(
                task_id="fetch_data",
                tool="web_search",
                execution_target="server",
                depends_on=[],
                inputs={"query": "latest AI research papers"},
                lifecycle_messages=LifecycleMessages(
                    on_start="ğŸ“¥ Fetching research data from web...",
                    on_success="âœ… Data fetched successfully"
                )
            ),
            Task(
                task_id="open_notepad",
                tool="open_app",
                execution_target="client",
                depends_on=[],
                inputs={"target": "notepad"},
                input_bindings={
                    "content": "$.tasks.step_1.output.data.query_demo" 
                },
                lifecycle_messages=LifecycleMessages(
                    on_start="ğŸ“ Opening Notepad on client...",
                    on_success="âœ… Notepad opened successfully"
                )
            ),
          
        ]

        # 4. Register Tasks
        logger.info(f"ğŸ“ [SQH] Registering {len(tasks)} tasks with Orchestrator...")
        orchestrator = get_orchestrator()
        await orchestrator.register_tasks(user_id, tasks)
        
        # 5. Setup Execution Dependencies
        execution_engine = get_execution_engine()
        
        # âœ… Ensure server executor is set
        if not execution_engine.server_tool_executor:
            logger.info("ğŸ”§ [SQH] Injecting server executor...")
            execution_engine.set_server_executor(get_server_executor())
        
        # âœ… Ensure client emitter is set AND configured
        if not execution_engine.client_task_emitter:
            logger.info("ğŸ”§ [SQH] Setting up client task emitter...")
            # This initializes emitter AND wires callback to client_core
            client_emitter = get_task_emitter()
            execution_engine.set_client_emitter(client_emitter)
        
        # 5. Trigger Execution Engine
        logger.info(f"âš¡ [SQH] Starting execution for {len(tasks)} tasks...")
        engine_task = await execution_engine.start_execution(user_id)
        await engine_task
        
        # 6. Get Summary
        summary = await orchestrator.get_execution_summary(user_id)
        logger.info(f"\nğŸ“Š Summary: {summary}")
    
        
        logger.info(f"âœ… [SQH] Execution workflow initiated for user: {user_id}")

    except Exception as e:
        logger.error(f"âŒ [SQH] Critical Failure: {e}", exc_info=True)